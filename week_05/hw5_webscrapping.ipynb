{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scrapping for Data Scientist job in CO (9 points total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will do web scrapping for **Data Scientist job in CO**\n",
    "\n",
    "\n",
    "Here is the link to the search query\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=CO\n",
    "\n",
    "As you can see at the bottom of the page there are links to several pages related to this search.\n",
    "If you click on second page, search url changes to\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=CO&start=10\n",
    "\n",
    "If you click on 3rd then url changes to\n",
    "\n",
    "https://www.indeed.com/jobs?q=data+scientist&l=CO&start=20\n",
    "\n",
    "Hence, to go to more pages we can format the search string(**change start=??** part) for **requests.get in a loop**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1(5 =  4(non indicator columns) + 1(indicator columns) points) Please complete the following task\n",
    "\n",
    "- Scrape 10 pages (**last page(10 th) url will be https://www.indeed.com/jobs?q=data+scientist&l=CO&start=90**)and build a pandas DataFrame containing following information\n",
    "    + **job title, name of the company, location, summary of job description**\n",
    "    + **Indicator columns(with value True/False) about keywords Python, SQL, AWS, RESTFUL, Machine learning, Deep Learning, Text Mining, NLP, SAS, Tableau, Sagemaker, TensorFlow, Spark**\n",
    "\n",
    "Note:\n",
    "- Make sure that you do a case insensitive search for keywords when filing(Tue/False) in the indicator columns\n",
    "- You need to go to the webpage of detail job posting for keywords search. The main job posting only contains summary of the job description.  Build detail job posting webpage url from web scrapping main search results.\n",
    "- If you run into difficulties which you are not able to overcome, skip this question and import the datafram from the provided the pickle file instead.\n",
    "- If you find this entire homework too difficult at your current level of expertise, please feel free to complete the AlternateHwk5 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling https://www.indeed.com/jobs?q=data+scientist&l=CO\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "html_link = \"https://www.indeed.com\"\n",
    "base_url = \"https://www.indeed.com\"\n",
    "base_search_url = f\"{base_url}/jobs?q=data+scientist&l=CO\"\n",
    "print(\"calling\",base_search_url)\n",
    "response = requests.get(base_search_url)\n",
    "zip_code_re = re.compile(r'[0-9]+')\n",
    "job_df = None\n",
    "job_keywords = {\"Python\":\"has_python\",\n",
    "                \"SQL\": \"has_sql\", \n",
    "                \"AWS\": \"has_aws\", \n",
    "                \"RESTFUL\": \"has_rest\", \n",
    "                \"Machine learning\": \"has_ml\", \n",
    "                \"Deep Learning\": \"has_dl\", \n",
    "                \"Text Mining\": \"has_mining\", \n",
    "                \"NLP\": \"has_nlp\", \n",
    "                \"SAS\": \"has_sas\", \n",
    "                \"Tableau\": \"has_tableau\", \n",
    "                \"Sagemaker\": \"has_sagemaker\", \n",
    "                \"TensorFlow\": \"has_tf\", \n",
    "                \"Spark\": \"has_spark\"}\n",
    "names = [\"job_title\",\"company\",\"location\",\"description\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "tables = soup.findAll('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_details_main(href_value):\n",
    "    \"\"\"\n",
    "    Extract the details from a job by making an http call to the specific job page\n",
    "    @param href_value the hyperlink url path\n",
    "    \"\"\"\n",
    "\n",
    " #   print(job_keywords)\n",
    "    job_url = f'{base_url}{href_value}'\n",
    "    job_response = requests.get(job_url)\n",
    "#    print(job_response.status_code)\n",
    "    if job_response.status_code == 200:\n",
    "        job_soup = BeautifulSoup(job_response.text)\n",
    "        content_container = job_soup.findAll(class_=\"jobsearch-ViewJobLayout-mainContent\")\n",
    "        main_content = content_container.pop()\n",
    "        # convert the bs4 NavigableString to base string, otherwise we get a pickle error later\n",
    "        job_title = str(main_content.find(\"h1\",class_=\"jobsearch-JobInfoHeader-title\").string)\n",
    "        # convert the bs4 NavigableString to base string, otherwise we get a pickle error later\n",
    "        company_name_div = main_content.findAll(\"div\",class_=\"icl-u-lg-mr--sm icl-u-xs-mr--xs\")\n",
    "        company_name = pd.NA\n",
    "        # we want the one that has a child a, but it doesn't seem to always work, so we set company_name to Na first\n",
    "        for d in company_name_div:\n",
    "            if d.find(\"a\"):\n",
    "                company_name = str(d.find(\"a\").string)\n",
    "        empty_divs = main_content.findAll(\"div\",class_=\"\")\n",
    "        # the location seems to be the 4th empty div tag\n",
    "        location = str(list(empty_divs)[4].string)\n",
    "        location = re.sub(zip_code_re,\"\",location)\n",
    "        location = location.replace(\",\", \" \")\n",
    "        location = location.strip()\n",
    "        # locations may have zip codes in them, so regex them out\n",
    "        job_description_div = main_content.find(class_=\"jobsearch-jobDescriptionText\")\n",
    "        \n",
    "        # there seems to be too much variation in how the job desciptions are structure to pick up a \"job summary\".\n",
    "        # we will just pull in the entire description\n",
    "        job_description = str(job_description_div.text)\n",
    "        job_dict = {\"job_title\":[job_title],\n",
    "                           \"company\":[company_name],\n",
    "                           \"location\": [location],\n",
    "                           \"description\": [job_description]}\n",
    "        for keyword, col_name in job_keywords.items():\n",
    "            job_dict[col_name] = keyword.lower() in job_description.lower()\n",
    "        # print(job_title,\"|\",company_name,\"|\",location,\"|\",job_description[0:30], \"|\", has_keyword)\n",
    "        return pd.DataFrame(job_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = None\n",
    "def process_one_page(links):\n",
    "    \"\"\"\n",
    "    Go through each link tag and look for ones that include /rc in the href value.\n",
    "    This will point to a job details link\n",
    "    @param a list of hyperlinks tags\n",
    "    \"\"\"\n",
    "    page_df = None\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "    #    print(href)\n",
    "        if href:\n",
    "    #        print(href)\n",
    "            if \"/rc\" in href:\n",
    "    #            print(\"rc: \",href)\n",
    "                df = get_job_details_main(href)\n",
    "    #            print(df)\n",
    "                if page_df is None:\n",
    "                    page_df = df;\n",
    "                else:\n",
    "                    page_df = page_df.append(df, ignore_index=True)\n",
    "    #           print(page_df[\"job_title\"])\n",
    "    return page_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pages(num_pages):\n",
    "    \"\"\"\n",
    "    Loop through the desired number of pages\n",
    "    @param num_pages the number of pages to process\n",
    "    \"\"\"\n",
    "    pages_df = None\n",
    "    for page_number in range(num_pages):\n",
    "        start_value = page_number*10\n",
    "        page_url = f'{base_search_url}&start={start_value}'\n",
    "        print(f'searching for page {page_number} with url {page_url}')\n",
    "        response = requests.get(base_search_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        html_links = soup.findAll('a')\n",
    "        print(f\"found {len(html_links)} links\")\n",
    "        df = process_one_page(html_links)\n",
    "        if pages_df is None:\n",
    "            print(\"setting\")\n",
    "            pages_df = df\n",
    "        else:\n",
    "            print(\"appending\")\n",
    "            pages_df = pages_df.append(df, ignore_index=True)\n",
    "        print(f'now have {pages_df.size} after page {page_number}')\n",
    "    return pages_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for page 0 with url https://www.indeed.com/jobs?q=data+scientist&l=CO&start=0\n",
      "found 189 links\n",
      "setting\n",
      "now have 187 after page 0\n",
      "searching for page 1 with url https://www.indeed.com/jobs?q=data+scientist&l=CO&start=10\n",
      "found 189 links\n",
      "appending\n",
      "now have 374 after page 1\n",
      "searching for page 2 with url https://www.indeed.com/jobs?q=data+scientist&l=CO&start=20\n",
      "found 198 links\n",
      "appending\n",
      "now have 561 after page 2\n",
      "searching for page 3 with url https://www.indeed.com/jobs?q=data+scientist&l=CO&start=30\n",
      "found 190 links\n",
      "appending\n",
      "now have 748 after page 3\n"
     ]
    }
   ],
   "source": [
    "indeed_job_df = process_pages(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2(1 point) Save you DataFrame to a pickle file name *indeed_job_co.pkl*. \n",
    "   Load this pkl file in dataFrame and use this dataFrame for answering following questions.\n",
    "\n",
    "   <font color='red'>upload the pickle file(indeed_job_co.pkl) along with solution notebook to the canvas</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code here\n",
    "# save the pickle file\n",
    "indeed_job_df.to_pickle(\"indeed_job_co.pkl\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>has_python</th>\n",
       "      <th>has_sql</th>\n",
       "      <th>has_aws</th>\n",
       "      <th>has_rest</th>\n",
       "      <th>has_ml</th>\n",
       "      <th>has_dl</th>\n",
       "      <th>has_mining</th>\n",
       "      <th>has_nlp</th>\n",
       "      <th>has_sas</th>\n",
       "      <th>has_tableau</th>\n",
       "      <th>has_sagemaker</th>\n",
       "      <th>has_tf</th>\n",
       "      <th>has_spark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Highlands Ranch  CO</td>\n",
       "      <td>\\n Company Description\\n  Visa is a world lead...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist (Junior)</td>\n",
       "      <td>BDSA</td>\n",
       "      <td>Louisville  CO</td>\n",
       "      <td>\\nJob Summary: The Analytics team empowers BDS...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ISSAC Corp</td>\n",
       "      <td>Colorado Springs  CO</td>\n",
       "      <td>\\nTop Reasons to work with us\\n\\nA small team ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>NomiSo</td>\n",
       "      <td>Englewood  CO</td>\n",
       "      <td>\\n\\n\\nLocation: Englewood, CO\\n \\n\\n About Nom...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist (3-5 years experience)</td>\n",
       "      <td>Datalab USA</td>\n",
       "      <td>Broomfield  CO</td>\n",
       "      <td>\\n\\n  DataLab USA\\n  ™ is an analytics and tec...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               job_title      company              location  \\\n",
       "0                         Data Scientist         Visa   Highlands Ranch  CO   \n",
       "1                Data Scientist (Junior)         BDSA        Louisville  CO   \n",
       "2                         Data Scientist   ISSAC Corp  Colorado Springs  CO   \n",
       "3                         Data Scientist       NomiSo         Englewood  CO   \n",
       "4  Data Scientist (3-5 years experience)  Datalab USA        Broomfield  CO   \n",
       "\n",
       "                                         description  has_python  has_sql  \\\n",
       "0  \\n Company Description\\n  Visa is a world lead...        True     True   \n",
       "1  \\nJob Summary: The Analytics team empowers BDS...        True    False   \n",
       "2  \\nTop Reasons to work with us\\n\\nA small team ...        True    False   \n",
       "3  \\n\\n\\nLocation: Englewood, CO\\n \\n\\n About Nom...        True    False   \n",
       "4  \\n\\n  DataLab USA\\n  ™ is an analytics and tec...        True     True   \n",
       "\n",
       "   has_aws  has_rest  has_ml  has_dl  has_mining  has_nlp  has_sas  \\\n",
       "0    False     False   False   False       False    False     True   \n",
       "1    False     False   False   False       False    False     True   \n",
       "2    False     False    True    True       False    False    False   \n",
       "3     True     False    True   False       False     True    False   \n",
       "4    False     False   False   False       False    False    False   \n",
       "\n",
       "   has_tableau  has_sagemaker  has_tf  has_spark  \n",
       "0         True          False   False      False  \n",
       "1        False          False   False      False  \n",
       "2        False          False   False      False  \n",
       "3        False           True    True       True  \n",
       "4         True          False   False      False  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read it back in\n",
    "job_df = pd.read_pickle(\"indeed_job_co.pkl\", compression=\"gzip\")\n",
    "job_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = \"6\" color='red'> Use pandas functionality to answer question 3</font>\n",
    "# Q 3 a(1 point) Which city has maximum job posting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "location\n",
       "Denver  CO    18\n",
       "Name: job_title, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.groupby([\"location\"])['job_title'].count().sort_values(ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q 3 b(1.5 point) - Top 3 most demanding skills(like Python, AWS, SQL ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_python    35\n",
       "has_ml        24\n",
       "has_sql       24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df[job_df==True].count().sort_values(ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 c(.5 point) What other questions you would like to ask  based on indeed data?\n",
    "\n",
    "This is a free response question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most in-demand skill pairings (all possible combinations of the indicator columns). With more data we could break that down by location.\n",
    "- \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
